import torch
# from pynvml.smi import nvidia_smi

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group["lr"]

# def print_gpu_memory_report():
#     if torch.cuda.is_available():
#         nvsmi = nvidia_smi.getInstance()
#         data = nvsmi.DeviceQuery("memory.used, memory.total, utilization.gpu")["gpu"]
#         print("Memory report")
#         for i, data_by_rank in enumerate(data):
#             mem_report = data_by_rank["fb_memory_usage"]
#             print(f"gpu:{i} mem(%) {int(mem_report['used'] * 100.0 / mem_report['total'])}")


